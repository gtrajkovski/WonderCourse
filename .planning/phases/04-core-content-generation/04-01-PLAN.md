---
phase: 04-core-content-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/generators/base_generator.py
  - src/generators/schemas/__init__.py
  - src/generators/schemas/video_script.py
  - src/generators/schemas/reading.py
  - src/generators/schemas/quiz.py
  - src/generators/schemas/rubric.py
  - src/utils/content_metadata.py
  - tests/test_content_metadata.py
autonomous: true

must_haves:
  truths:
    - "BaseGenerator ABC defines abstract interface for all content generators"
    - "ContentMetadata calculates word count and duration deterministically"
    - "Pydantic schemas exist for all 4 content types (video, reading, quiz, rubric)"
  artifacts:
    - path: "src/generators/base_generator.py"
      provides: "Abstract base class with generate(), system_prompt, build_user_prompt, extract_metadata"
      contains: "class BaseGenerator"
    - path: "src/utils/content_metadata.py"
      provides: "Word count, video duration (150 WPM), reading duration (238 WPM), quiz duration (1.5 min/q)"
      contains: "class ContentMetadata"
    - path: "src/generators/schemas/video_script.py"
      provides: "VideoScriptSchema with WWHAA section fields"
      contains: "class VideoScriptSchema"
    - path: "src/generators/schemas/reading.py"
      provides: "ReadingSchema with sections and references"
      contains: "class ReadingSchema"
    - path: "src/generators/schemas/quiz.py"
      provides: "QuizSchema with questions, distractors, feedback"
      contains: "class QuizSchema"
    - path: "src/generators/schemas/rubric.py"
      provides: "RubricSchema with 3-level scoring criteria"
      contains: "class RubricSchema"
  key_links:
    - from: "src/generators/base_generator.py"
      to: "anthropic.Anthropic"
      via: "creates own client in __init__"
      pattern: "self\\.client = Anthropic"
    - from: "src/generators/base_generator.py"
      to: "src/config.py"
      via: "Config.ANTHROPIC_API_KEY, Config.MODEL, Config.MAX_TOKENS"
      pattern: "from src.config import Config"
---

<objective>
Create the BaseGenerator abstract base class, ContentMetadata utility, and all 4 Pydantic schema models that define the structured output format for each content type.

Purpose: Establish the shared infrastructure that all 4 content generators (Plans 02-05) depend on. This must be complete before any generator can be built.
Output: BaseGenerator ABC, ContentMetadata utility with tests, and 4 Pydantic schema files.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-core-content-generation/04-RESEARCH.md
@src/generators/blueprint_generator.py
@src/core/models.py
@src/config.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create BaseGenerator ABC and ContentMetadata utility</name>
  <files>
    src/generators/base_generator.py
    src/utils/content_metadata.py
    tests/test_content_metadata.py
  </files>
  <action>
Create `src/generators/base_generator.py` with:

```python
from abc import ABC, abstractmethod
from typing import TypeVar, Generic, Tuple
from pydantic import BaseModel
from anthropic import Anthropic
from src.config import Config

T = TypeVar('T', bound=BaseModel)

class BaseGenerator(ABC, Generic[T]):
```

Abstract methods:
- `system_prompt` property (abstract) -> str: Generator-specific system instructions
- `build_user_prompt(self, **kwargs) -> str`: Build prompt from input parameters
- `extract_metadata(self, content: T) -> dict`: Calculate word_count, estimated_duration_minutes, etc.

Concrete method:
- `generate(self, schema: type[T], **prompt_kwargs) -> Tuple[T, dict]`: Calls Claude API with structured outputs using `output_config.format` parameter with `schema.model_json_schema()`, validates response with `schema.model_validate_json()`, then calls `self.extract_metadata()` and returns tuple of (content, metadata).

Constructor `__init__(self, api_key=None, model=None)`:
- Creates own Anthropic client: `self.client = Anthropic(api_key=api_key or Config.ANTHROPIC_API_KEY)`
- Sets model: `self.model = model or Config.MODEL`
- Uses `Config.MAX_TOKENS` for max_tokens in API call

Follow the pattern from `BlueprintGenerator` in `src/generators/blueprint_generator.py` but as an abstract base class.

Create `src/utils/content_metadata.py` with:

```python
class ContentMetadata:
    WPM_READING = 238    # Adult non-fiction average
    WPM_SPEAKING = 150   # Video delivery rate
    MINUTES_PER_QUIZ_QUESTION = 1.5

    @staticmethod
    def count_words(text: str) -> int

    @staticmethod
    def estimate_reading_duration(word_count: int) -> float  # returns rounded to 1 decimal

    @staticmethod
    def estimate_video_duration(word_count: int) -> float  # returns rounded to 1 decimal

    @staticmethod
    def estimate_quiz_duration(num_questions: int) -> float  # 1.5 min per question
```

All duration methods return minutes rounded to 1 decimal place.

Create `tests/test_content_metadata.py` with tests for:
- `count_words("")` returns 0
- `count_words("hello world")` returns 2
- `estimate_reading_duration(238)` returns 1.0
- `estimate_reading_duration(476)` returns 2.0
- `estimate_video_duration(150)` returns 1.0
- `estimate_video_duration(750)` returns 5.0
- `estimate_quiz_duration(5)` returns 7.5
- `estimate_quiz_duration(10)` returns 15.0
  </action>
  <verify>Run `python -m pytest tests/test_content_metadata.py -v` and confirm all 8+ tests pass.</verify>
  <done>BaseGenerator ABC importable, ContentMetadata utility works with deterministic calculations, all tests pass.</done>
</task>

<task type="auto">
  <name>Task 2: Create Pydantic schema models for all 4 content types</name>
  <files>
    src/generators/schemas/__init__.py
    src/generators/schemas/video_script.py
    src/generators/schemas/reading.py
    src/generators/schemas/quiz.py
    src/generators/schemas/rubric.py
  </files>
  <action>
Create `src/generators/schemas/__init__.py` that imports all schema classes.

Create `src/generators/schemas/video_script.py`:
```python
class VideoScriptSection(BaseModel):
    """Single WWHAA section of a video script."""
    phase: Literal["hook", "objective", "content", "ivq", "summary", "cta"]
    title: str = Field(description="Section heading")
    script_text: str = Field(description="Narration/dialogue for this section")
    speaker_notes: str = Field(description="Delivery guidance for instructor")

class VideoScriptSchema(BaseModel):
    """Complete WWHAA video script."""
    title: str = Field(description="Video title")
    hook: VideoScriptSection
    objective: VideoScriptSection
    content: VideoScriptSection
    ivq: VideoScriptSection  # In-video question
    summary: VideoScriptSection
    cta: VideoScriptSection  # Call to action
    learning_objective: str = Field(description="The learning objective this video addresses")
```

Each section is a separate field (not a list) so WWHAA structure is enforced by schema. The `phase` field on each section is fixed by the generator prompt.

Create `src/generators/schemas/reading.py`:
```python
class ReadingSection(BaseModel):
    heading: str = Field(description="Section heading")
    body: str = Field(description="Section content text")

class Reference(BaseModel):
    citation: str = Field(description="APA 7 formatted citation")
    url: str = Field(description="URL or DOI if available", default="")

class ReadingSchema(BaseModel):
    title: str
    introduction: str = Field(description="Opening paragraph, 50-100 words")
    sections: List[ReadingSection] = Field(min_length=2, max_length=6, description="Main content sections")
    conclusion: str = Field(description="Closing paragraph, 50-100 words")
    references: List[Reference] = Field(min_length=1, max_length=5, description="APA 7 references")
    learning_objective: str
```

Create `src/generators/schemas/quiz.py`:
```python
class QuizOption(BaseModel):
    text: str = Field(description="Answer option text")
    is_correct: bool = Field(description="Whether this is the correct answer")
    feedback: str = Field(description="Feedback shown when this option is selected")

class QuizQuestion(BaseModel):
    question_text: str = Field(description="Question stem")
    options: List[QuizOption] = Field(min_length=3, max_length=4, description="Answer options (1 correct, 2-3 distractors)")
    bloom_level: Literal["remember", "understand", "apply", "analyze", "evaluate", "create"]
    explanation: str = Field(description="Detailed explanation of correct answer")

class QuizSchema(BaseModel):
    title: str
    questions: List[QuizQuestion] = Field(min_length=3, max_length=10)
    passing_score_percentage: int = Field(description="Passing threshold, typically 70")
    learning_objective: str
```

Note: Using `QuizOption` with `is_correct` field instead of separate correct_answer + distractors. This way the answer position varies naturally in the schema and we can validate balanced distribution.

Create `src/generators/schemas/rubric.py`:
```python
class RubricCriterion(BaseModel):
    name: str = Field(description="Criterion name (e.g., 'Content Quality')")
    description: str = Field(description="What this criterion evaluates")
    below_expectations: str = Field(description="Description of below expectations performance")
    meets_expectations: str = Field(description="Description of meets expectations performance")
    exceeds_expectations: str = Field(description="Description of exceeds expectations performance")
    weight_percentage: int = Field(description="Weight as percentage of total score")

class RubricSchema(BaseModel):
    title: str
    criteria: List[RubricCriterion] = Field(min_length=2, max_length=6, description="Scoring criteria")
    total_points: int = Field(description="Maximum points for this rubric")
    learning_objective: str
```

3-level scoring (Below/Meets/Exceeds Expectations) per criterion with descriptive text. This matches the research recommendation.
  </action>
  <verify>Run `python -c "from src.generators.schemas import VideoScriptSchema, ReadingSchema, QuizSchema, RubricSchema; print('All schemas importable'); print(QuizSchema.model_json_schema().keys())"` to confirm all schemas are importable and generate valid JSON schemas.</verify>
  <done>All 4 Pydantic schema models importable, each generates valid JSON schema via model_json_schema(), schemas enforce WWHAA structure for video and 3-level rubric scoring.</done>
</task>

</tasks>

<verification>
1. `python -m pytest tests/test_content_metadata.py -v` - All metadata tests pass
2. `python -c "from src.generators.base_generator import BaseGenerator"` - BaseGenerator importable
3. `python -c "from src.generators.schemas import VideoScriptSchema, ReadingSchema, QuizSchema, RubricSchema"` - All schemas importable
4. `python -c "from src.generators.schemas.quiz import QuizSchema; import json; print(json.dumps(QuizSchema.model_json_schema(), indent=2)[:200])"` - Schemas produce valid JSON schemas
5. `python -m pytest tests/ -v` - All existing tests still pass (no regressions)
</verification>

<success_criteria>
- BaseGenerator ABC with generate(), system_prompt, build_user_prompt, extract_metadata methods
- ContentMetadata with count_words, estimate_reading_duration, estimate_video_duration, estimate_quiz_duration
- All 4 Pydantic schemas importable and producing valid JSON schemas
- 8+ ContentMetadata tests passing
- No regressions in existing 180 tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-content-generation/04-01-SUMMARY.md`
</output>
