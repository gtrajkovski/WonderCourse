---
phase: 04-core-content-generation
plan: 05
type: tdd
wave: 2
depends_on: ["04-01"]
files_modified:
  - src/generators/rubric_generator.py
  - tests/test_rubric_generator.py
autonomous: true

must_haves:
  truths:
    - "RubricGenerator produces rubrics with 3-level scoring criteria (Below/Meets/Exceeds Expectations)"
    - "Each criterion has descriptive text for all 3 performance levels and a weight percentage"
    - "Metadata includes total_criteria count and total_points"
  artifacts:
    - path: "src/generators/rubric_generator.py"
      provides: "RubricGenerator extending BaseGenerator"
      contains: "class RubricGenerator"
    - path: "tests/test_rubric_generator.py"
      provides: "Tests with mocked Anthropic API"
      contains: "def test_"
  key_links:
    - from: "src/generators/rubric_generator.py"
      to: "src/generators/base_generator.py"
      via: "extends BaseGenerator"
      pattern: "class RubricGenerator\\(BaseGenerator"
    - from: "src/generators/rubric_generator.py"
      to: "src/generators/schemas/rubric.py"
      via: "uses RubricSchema"
      pattern: "from src.generators.schemas.rubric import"
---

<objective>
Implement RubricGenerator using TDD, producing assessment rubrics with 3-level scoring criteria (Below Expectations / Meets Expectations / Exceeds Expectations) for graded activities.

Purpose: Enable rubric generation for graded assignments, projects, and peer reviews - essential for assessment transparency.
Output: Working RubricGenerator with full test coverage via mocked API.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/STATE.md
@.planning/phases/04-core-content-generation/04-RESEARCH.md
@.planning/phases/04-core-content-generation/04-01-SUMMARY.md
@src/generators/base_generator.py
@src/generators/schemas/rubric.py
@src/utils/content_metadata.py
@src/config.py
</context>

<feature>
  <name>RubricGenerator with 3-level scoring criteria</name>
  <files>src/generators/rubric_generator.py, tests/test_rubric_generator.py</files>
  <behavior>
    RubricGenerator extends BaseGenerator[RubricSchema].

    Input: learning_objective (str), activity_title (str), activity_type (str), num_criteria (int, default 4), total_points (int, default 100)
    Output: (RubricSchema, metadata_dict) tuple

    Behavior cases:
    - generate(schema=RubricSchema, ...) -> returns RubricSchema with 2-6 RubricCriterion items
    - Each criterion has name, description, below_expectations, meets_expectations, exceeds_expectations, weight_percentage
    - Criterion weights should sum to 100 (validated in metadata)
    - extract_metadata() returns {"word_count": N, "total_criteria": M, "total_points": P, "weights_valid": bool}
    - Word count sums all criterion text (name, description, 3 level descriptions)
    - system_prompt includes rubric design best practices and 3-level scoring structure
    - build_user_prompt() includes learning_objective, activity_title, activity_type, num_criteria, total_points
  </behavior>
  <implementation>
    Create RubricGenerator class:
    - system_prompt property: Expert assessment rubric designer, 3-level analytic rubric conventions (Below/Meets/Exceeds Expectations), descriptive criteria that are observable and measurable, weight distribution guidance, avoid vague language
    - build_user_prompt(): Takes learning_objective, activity_title, activity_type, num_criteria, total_points. Format as CONTEXT/TASK. Include "Create a {num_criteria}-criterion rubric with {total_points} total points"
    - extract_metadata(): Count words across all criterion fields. Count total criteria. Check if weights sum to 100 (weights_valid). Include total_points from schema.
    - Convenience method generate_rubric(learning_objective, activity_title, activity_type, num_criteria, total_points)

    Tests (use pytest-mock):
    1. test_generate_returns_valid_schema - Mock API returns valid RubricSchema JSON, verify criteria present
    2. test_criteria_have_three_levels - Verify each criterion has below/meets/exceeds text
    3. test_system_prompt_contains_scoring_levels - Verify system_prompt mentions Below/Meets/Exceeds Expectations
    4. test_build_user_prompt_includes_activity_type - Verify activity_type and num_criteria in prompt
    5. test_extract_metadata_counts_criteria - Create RubricSchema with 3 criteria, verify total_criteria=3
    6. test_extract_metadata_validates_weights - Create rubric where weights sum to 100, verify weights_valid=True
    7. test_extract_metadata_detects_invalid_weights - Create rubric where weights sum to 90, verify weights_valid=False
    8. test_api_called_with_output_config - Verify messages.create uses output_config parameter

    Mock JSON response should include 3 criteria with name, description, 3 level descriptions, and weight_percentage fields.
  </implementation>
</feature>

<verification>
1. `python -m pytest tests/test_rubric_generator.py -v` - All tests pass
2. `python -m pytest tests/ -v` - No regressions
</verification>

<success_criteria>
- RubricGenerator extends BaseGenerator and generates 3-level rubrics
- 8+ tests passing with mocked API
- Each criterion has Below/Meets/Exceeds Expectations descriptions
- Metadata validates weight distribution
- No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/04-core-content-generation/04-05-SUMMARY.md`
</output>
