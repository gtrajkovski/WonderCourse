---
phase: 03-blueprint-generation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - requirements.txt
  - src/generators/__init__.py
  - src/generators/blueprint_generator.py
  - tests/test_blueprint_generator.py
autonomous: true

must_haves:
  truths:
    - "BlueprintGenerator produces valid CourseBlueprint JSON from course description and learning outcomes"
    - "Generated blueprint includes modules, lessons, activities with WWHAA phases and Bloom levels"
    - "Blueprint generation uses Claude structured outputs via output_config parameter for guaranteed schema compliance"
  artifacts:
    - path: "src/generators/blueprint_generator.py"
      provides: "BlueprintGenerator class with Pydantic schemas and AI generation"
      contains: "class BlueprintGenerator"
    - path: "src/generators/__init__.py"
      provides: "Package init"
    - path: "tests/test_blueprint_generator.py"
      provides: "Unit tests for blueprint generation with mocked AI"
      contains: "test_generate_blueprint"
  key_links:
    - from: "src/generators/blueprint_generator.py"
      to: "anthropic.Anthropic"
      via: "structured outputs API call with output_config"
      pattern: "output_config.*json_schema"
    - from: "src/generators/blueprint_generator.py"
      to: "src/config.py"
      via: "Config.ANTHROPIC_API_KEY and Config.MODEL"
      pattern: "from src.config import Config"
---

<objective>
Create the BlueprintGenerator class that uses Claude's structured outputs API with Pydantic schemas to generate complete course blueprints from high-level inputs.

Purpose: This is the core AI generation engine for Phase 3. It transforms a course description, learning outcomes, target duration, and audience level into a hierarchical module/lesson/activity structure with WWHAA phases and Bloom's taxonomy levels.

Output: `src/generators/blueprint_generator.py` with Pydantic models (ActivityBlueprint, LessonBlueprint, ModuleBlueprint, CourseBlueprint), prompt templates, and BlueprintGenerator class. Plus unit tests with mocked API responses.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-blueprint-generation/03-RESEARCH.md
@src/config.py
@src/ai/client.py
@src/core/models.py
@requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Pydantic dependency and create generators package</name>
  <files>requirements.txt, src/generators/__init__.py</files>
  <action>
1. Add `pydantic>=2.10.0` to requirements.txt under Core dependencies (after python-dotenv).
2. Create `src/generators/__init__.py` as empty package init file.
3. Run `py -3 -m pip install pydantic>=2.10.0` to install the dependency.

Keep it minimal - just the dependency addition and package creation.
  </action>
  <verify>Run `py -3 -c "import pydantic; print(pydantic.__version__)"` to confirm Pydantic is installed.</verify>
  <done>Pydantic is importable and listed in requirements.txt. src/generators/ package exists.</done>
</task>

<task type="auto">
  <name>Task 2: Create BlueprintGenerator with Pydantic schemas and prompt templates</name>
  <files>src/generators/blueprint_generator.py</files>
  <action>
Create `src/generators/blueprint_generator.py` with:

**Pydantic models (use these exact schemas):**

1. `ActivityBlueprint(BaseModel)`:
   - `title: str` (Field, description="Activity title", max_length=100)
   - `content_type: Literal["video", "reading", "quiz", "hol", "lab", "discussion", "assignment", "project"]`
   - `activity_type: Literal["video_lecture", "reading_material", "graded_quiz", "practice_quiz", "hands_on_lab", "coach_dialogue", "ungraded_lab", "discussion_prompt", "assignment_submission", "project_milestone"]`
   - `wwhaa_phase: Optional[Literal["hook", "objective", "content", "ivq", "summary", "cta"]]` (default None)
   - `bloom_level: Literal["remember", "understand", "apply", "analyze", "evaluate", "create"]`
   - `estimated_duration_minutes: float` (Field, ge=2.0, le=60.0)
   - `description: str` (Field, description="1-2 sentence description")

2. `LessonBlueprint(BaseModel)`:
   - `title: str`
   - `description: str`
   - `activities: List[ActivityBlueprint]`

3. `ModuleBlueprint(BaseModel)`:
   - `title: str`
   - `description: str`
   - `lessons: List[LessonBlueprint]`

4. `CourseBlueprint(BaseModel)`:
   - `modules: List[ModuleBlueprint]`
   - `total_duration_minutes: float`
   - `content_distribution: Dict[str, float]` (Field, description="Percentage breakdown by content_type")
   - `rationale: str` (Field, description="Brief explanation of design decisions")

**BlueprintGenerator class:**

```python
class BlueprintGenerator:
    """Generates course blueprints using Claude structured outputs."""

    SYSTEM_PROMPT = """..."""  # Use the educational prompt from research (Pattern 3)
    # Include WWHAA pedagogy, Bloom's taxonomy, content distribution targets,
    # Coursera requirements (2-3 modules, 3-5 lessons/module, 2-4 activities/lesson),
    # and DURATION_GUIDELINES from research

    def __init__(self, api_key: str = None, model: str = None):
        """Initialize with optional API key and model overrides.
        Falls back to Config values if not provided."""
        from anthropic import Anthropic
        self.client = Anthropic(api_key=api_key or Config.ANTHROPIC_API_KEY)
        self.model = model or Config.MODEL

    def generate(
        self,
        course_description: str,
        learning_outcomes: List[str],
        target_duration_minutes: int = 90,
        audience_level: str = "intermediate"
    ) -> CourseBlueprint:
        """Generate blueprint with structured outputs. Returns CourseBlueprint."""
        # Build prompt using _build_prompt()
        # CRITICAL: Use output_config parameter (NOT response_format)
        # Call self.client.messages.create() with:
        #   output_config={
        #       "format": {
        #           "type": "json_schema",
        #           "schema": CourseBlueprint.model_json_schema()
        #       }
        #   }
        # Parse response with CourseBlueprint.model_validate_json()
        # Return parsed blueprint

    def _build_prompt(self, description, outcomes, duration, level) -> str:
        """Build user prompt with CONTEXT-TASK structure."""
        # Format outcomes as numbered list
        # Include course context (description, audience, duration)
        # Include task instructions
```

IMPORTANT design decisions:
- BlueprintGenerator creates its OWN Anthropic client directly (not using AIClient from src/ai/client.py). This avoids conversation history pollution and keeps the generator stateless. The existing AIClient is designed for conversational use; the generator needs one-shot structured outputs.
- Use `Config.ANTHROPIC_API_KEY` and `Config.MODEL` as defaults.
- The `generate()` method returns a Pydantic model, not a dict. Callers use `.model_dump()` for JSON serialization.
- MAX_TOKENS should be 8192 for blueprint generation (blueprints are large).
- CRITICAL: The Anthropic SDK (v0.77.0+) uses `output_config` parameter. Do NOT use `response_format` (that is OpenAI's API). The correct call is:
  ```python
  response = self.client.messages.create(
      model=self.model,
      max_tokens=8192,
      system=self.SYSTEM_PROMPT,
      messages=[{"role": "user", "content": user_prompt}],
      output_config={
          "format": {
              "type": "json_schema",
              "schema": CourseBlueprint.model_json_schema()
          }
      }
  )
  ```
  </action>
  <verify>Run `py -3 -c "from src.generators.blueprint_generator import BlueprintGenerator, CourseBlueprint, ActivityBlueprint; print('Import OK')"` to confirm all classes are importable.</verify>
  <done>BlueprintGenerator class exists with Pydantic schemas, system prompt, and generate() method using output_config parameter. All classes importable without errors.</done>
</task>

<task type="auto">
  <name>Task 3: Create unit tests for BlueprintGenerator with mocked AI</name>
  <files>tests/test_blueprint_generator.py</files>
  <action>
Create `tests/test_blueprint_generator.py` with tests that mock the Anthropic API (following the pattern from `tests/test_ai_client.py`):

**Test helper:** Create a `SAMPLE_BLUEPRINT_JSON` string that is valid CourseBlueprint JSON. Include 2 modules, 3 lessons each, 2-3 activities per lesson, with realistic titles, types, durations, and Bloom levels. Total duration ~90 minutes.

**Tests to write:**

1. `test_course_blueprint_schema_valid()` - Verify CourseBlueprint.model_validate_json(SAMPLE_BLUEPRINT_JSON) parses correctly. Check module count, lesson count, activity types, Bloom levels.

2. `test_activity_blueprint_fields()` - Create ActivityBlueprint directly, check all fields. Test that wwhaa_phase is optional (None for non-video activities).

3. `test_generate_blueprint(mocker)` - Mock `anthropic.Anthropic` and its `messages.create()` to return SAMPLE_BLUEPRINT_JSON. Call `generator.generate()` with course description and outcomes. Assert result is a CourseBlueprint with expected module count.

4. `test_generate_blueprint_prompt_includes_description(mocker)` - Mock API, call generate(), capture the messages arg passed to messages.create(). Assert course description appears in the user message content.

5. `test_generate_blueprint_prompt_includes_outcomes(mocker)` - Similar to above, assert learning outcomes appear in user message.

6. `test_generate_blueprint_uses_output_config(mocker)` - Mock API, call generate(), assert `output_config` kwarg was passed to messages.create() with format type "json_schema". CRITICAL: Check for `output_config` NOT `response_format`.

7. `test_build_prompt_format()` - Call `generator._build_prompt()` directly. Assert it includes course description, audience level, target duration, and formatted outcomes.

**Mocking pattern:**
```python
mock_client = mocker.MagicMock()
mock_response = mocker.MagicMock()
mock_response.content = [mocker.MagicMock(text=SAMPLE_BLUEPRINT_JSON)]
mock_client.messages.create.return_value = mock_response
mocker.patch('src.generators.blueprint_generator.Anthropic', return_value=mock_client)

generator = BlueprintGenerator(api_key="test-key")
```

**CRITICAL test for structured outputs parameter:**
```python
def test_generate_blueprint_uses_output_config(mocker):
    """Verify generate() passes output_config (NOT response_format) to SDK."""
    # ... mock setup ...
    generator.generate(...)
    call_kwargs = mock_client.messages.create.call_args
    assert 'output_config' in call_kwargs.kwargs
    assert call_kwargs.kwargs['output_config']['format']['type'] == 'json_schema'
    # Ensure response_format is NOT used
    assert 'response_format' not in call_kwargs.kwargs
```
  </action>
  <verify>Run `py -3 -m pytest tests/test_blueprint_generator.py -v` and confirm all tests pass.</verify>
  <done>All 7 BlueprintGenerator tests pass. Tests cover schema validation, generation with mocked API, prompt construction, and output_config structured outputs usage.</done>
</task>

</tasks>

<verification>
1. `py -3 -c "from src.generators.blueprint_generator import BlueprintGenerator, CourseBlueprint"` succeeds
2. `py -3 -m pytest tests/test_blueprint_generator.py -v` — all tests pass
3. `py -3 -m pytest tests/ -v` — all 142+ existing tests still pass (no regressions)
</verification>

<success_criteria>
- BlueprintGenerator class exists with generate() method using Claude structured outputs via output_config parameter
- Pydantic schemas define ActivityBlueprint, LessonBlueprint, ModuleBlueprint, CourseBlueprint
- System prompt includes WWHAA pedagogy, Bloom's taxonomy, content distribution, and duration guidelines
- 7+ tests pass covering schema validation, generation, prompt construction, output_config structured outputs
- Test explicitly verifies output_config is used (NOT response_format)
- No regressions in existing 142 tests
</success_criteria>

<output>
After completion, create `.planning/phases/03-blueprint-generation/03-01-SUMMARY.md`
</output>
