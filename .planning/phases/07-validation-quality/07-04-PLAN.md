---
phase: 07-validation-quality
plan: 04
type: tdd
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/validators/distractor_validator.py
  - tests/test_distractor_validator.py
autonomous: true

must_haves:
  truths:
    - "System validates quiz distractor quality"
    - "Error if question has multiple correct answers"
    - "Error if question has no correct answer"
    - "Error if distractor too similar to correct answer (>85% similarity)"
    - "Warning if question has fewer than 2 distractors"
  artifacts:
    - path: "src/validators/distractor_validator.py"
      provides: "DistractorValidator class"
      exports: ["DistractorValidator"]
    - path: "tests/test_distractor_validator.py"
      provides: "DistractorValidator tests"
      min_lines: 80
  key_links:
    - from: "src/validators/distractor_validator.py"
      to: "src/validators/validation_result.py"
      via: "import"
      pattern: "from src.validators.validation_result import ValidationResult"
---

<objective>
Create DistractorValidator to analyze quiz question distractor quality.

Purpose: Implements QA-06 (distractor quality analysis). Detects obvious issues like duplicate correct answers, missing correct answers, distractors too similar to correct answer, and questions with too few distractors.

Output: DistractorValidator class with validate_quiz() method that returns ValidationResult with distractor quality analysis.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/07-validation-quality/07-RESEARCH.md
@.planning/phases/07-validation-quality/07-01-PLAN.md
@src/validators/validation_result.py
@src/generators/schemas/quiz.py
</context>

<feature>
  <name>DistractorValidator for quiz question quality</name>
  <files>src/validators/distractor_validator.py, tests/test_distractor_validator.py</files>
  <behavior>
    DistractorValidator.validate_quiz(quiz_content: str) returns ValidationResult with:

    Input: JSON string of quiz content (parseable as QuizSchema)

    Cases per question:
    - 2+ options marked is_correct=True -> error "Q{n}: Multiple correct answers"
    - 0 options marked is_correct=True -> error "Q{n}: No correct answer"
    - Distractor text >85% similar to correct answer -> error "Q{n}: Distractor too similar"
    - Only 1 distractor (2 options total) -> warning "Q{n}: Only 1 distractor (recommended 2-3)"
    - Distractor text less than 5 chars -> error "Q{n}: Implausible distractor too short"
    - Invalid JSON -> error "Invalid quiz content"

    Metrics:
    - total_questions: int
    - flagged_questions: int (questions with any issue)
    - distractor_quality_score: float (1.0 - flagged/total)
  </behavior>
  <implementation>
Create DistractorValidator class:

```python
import json
from src.validators.validation_result import ValidationResult
from src.generators.schemas.quiz import QuizSchema

class DistractorValidator:
    """Validates quiz distractor quality."""

    SIMILARITY_THRESHOLD = 0.85  # Flag if distractor >85% similar to correct
    MIN_DISTRACTORS = 2  # Recommended minimum distractors per question
    MIN_OPTION_LENGTH = 5  # Minimum characters for plausible option

    def validate_quiz(self, quiz_content: str) -> ValidationResult:
        """Validate distractors in quiz JSON content."""
        errors = []
        warnings = []
        suggestions = []

        # Parse quiz content
        try:
            quiz_data = json.loads(quiz_content)
            quiz = QuizSchema.model_validate(quiz_data)
        except Exception as e:
            return ValidationResult(
                is_valid=False,
                errors=[f"Invalid quiz content: {e}"],
                warnings=[],
                suggestions=[],
                metrics={}
            )

        total_questions = len(quiz.questions)
        flagged_questions = 0

        for i, question in enumerate(quiz.questions, 1):
            question_has_issues = False

            # Check 1: Correct answer count
            correct_count = sum(1 for opt in question.options if opt.is_correct)
            if correct_count > 1:
                errors.append(f"Q{i}: Multiple correct answers ({correct_count})")
                question_has_issues = True
            elif correct_count == 0:
                errors.append(f"Q{i}: No correct answer marked")
                question_has_issues = True

            # Check 2: Distractor similarity to correct
            correct_option = next((opt for opt in question.options if opt.is_correct), None)
            if correct_option:
                correct_text = correct_option.text.lower().strip()
                for opt in question.options:
                    if not opt.is_correct:
                        similarity = self._calculate_similarity(correct_text, opt.text.lower().strip())
                        if similarity > self.SIMILARITY_THRESHOLD:
                            option_preview = opt.text[:30] + "..." if len(opt.text) > 30 else opt.text
                            errors.append(f"Q{i}: Distractor too similar to correct answer ('{option_preview}')")
                            question_has_issues = True

            # Check 3: Distractor count
            distractor_count = len([opt for opt in question.options if not opt.is_correct])
            if distractor_count < self.MIN_DISTRACTORS:
                warnings.append(f"Q{i}: Only {distractor_count} distractor(s) (recommended {self.MIN_DISTRACTORS}+)")

            # Check 4: Implausible short distractors
            for opt in question.options:
                if not opt.is_correct and len(opt.text.strip()) < self.MIN_OPTION_LENGTH:
                    errors.append(f"Q{i}: Implausible distractor too short ('{opt.text}')")
                    question_has_issues = True

            if question_has_issues:
                flagged_questions += 1

        # Build metrics
        quality_score = 1.0 - (flagged_questions / total_questions) if total_questions > 0 else 0.0

        metrics = {
            "total_questions": total_questions,
            "flagged_questions": flagged_questions,
            "distractor_quality_score": round(quality_score, 2)
        }

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings,
            suggestions=suggestions,
            metrics=metrics
        )

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        """Calculate word-level Jaccard similarity (0.0 to 1.0)."""
        if not text1 or not text2:
            return 0.0

        words1 = set(text1.split())
        words2 = set(text2.split())

        if not words1 or not words2:
            return 0.0

        intersection = len(words1 & words2)
        union = len(words1 | words2)

        return intersection / union if union > 0 else 0.0
```
  </implementation>
</feature>

<verification>
RED phase:
- Write tests first that fail
- Run pytest tests/test_distractor_validator.py - tests must FAIL

GREEN phase:
- Implement DistractorValidator
- Run pytest tests/test_distractor_validator.py - tests must PASS

REFACTOR phase (if needed):
- Clean up implementation
- Run pytest tests/test_distractor_validator.py - tests still PASS
</verification>

<success_criteria>
- DistractorValidator.validate_quiz() returns ValidationResult
- Multiple correct answers produces error
- No correct answer produces error
- Similar distractor produces error
- Few distractors produces warning
- Short distractor produces error
- Invalid JSON produces error with clear message
- Metrics include quality score
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-validation-quality/07-04-SUMMARY.md`
</output>
