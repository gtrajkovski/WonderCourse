---
phase: 07-validation-quality
plan: 02
type: tdd
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/validators/outcome_validator.py
  - tests/test_outcome_validator.py
autonomous: true

must_haves:
  truths:
    - "System identifies unmapped learning outcomes (no activities linked)"
    - "System identifies low-coverage outcomes (fewer than 2 activities)"
    - "System identifies unmapped activities (not linked to any outcome)"
    - "Coverage score calculated as percentage of outcomes with at least one activity"
  artifacts:
    - path: "src/validators/outcome_validator.py"
      provides: "OutcomeValidator class"
      exports: ["OutcomeValidator"]
    - path: "tests/test_outcome_validator.py"
      provides: "OutcomeValidator tests"
      min_lines: 80
  key_links:
    - from: "src/validators/outcome_validator.py"
      to: "src/validators/validation_result.py"
      via: "import"
      pattern: "from src.validators.validation_result import ValidationResult"
    - from: "src/validators/outcome_validator.py"
      to: "src/core/models.py"
      via: "import"
      pattern: "from src.core.models import Course"
---

<objective>
Create OutcomeValidator to detect alignment gaps between learning outcomes and activities.

Purpose: Implements QA-03 (outcome-activity alignment with coverage scoring) and QA-04 (gap detection). Users need to know which outcomes are not covered by activities and which activities are orphaned (not linked to any outcome).

Output: OutcomeValidator class with validate() method that returns ValidationResult with gap analysis and coverage metrics.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/07-validation-quality/07-RESEARCH.md
@.planning/phases/07-validation-quality/07-01-PLAN.md
@src/validators/validation_result.py
@src/core/models.py
@src/api/learning_outcomes.py
</context>

<feature>
  <name>OutcomeValidator for coverage and gap detection</name>
  <files>src/validators/outcome_validator.py, tests/test_outcome_validator.py</files>
  <behavior>
    OutcomeValidator.validate(course) returns ValidationResult with:

    Cases:
    - Course with no outcomes -> is_valid=True, warning "No learning outcomes defined"
    - Outcome with 0 mapped activities -> error "Unmapped outcome: {behavior}"
    - Outcome with 1 mapped activity -> warning "Low coverage: {behavior} has only 1 activity"
    - Outcome with 2+ valid activities -> no issue
    - Activity not in any outcome.mapped_activity_ids -> warning "Unmapped activities: N"
    - Stale activity ID in mapped_activity_ids (activity deleted) -> filtered out, not counted

    Metrics:
    - coverage_score: float (0.0-1.0) = outcomes with 1+ activities / total outcomes
    - unmapped_outcomes: int
    - low_coverage_outcomes: int
    - unmapped_activities: int
    - avg_activities_per_outcome: float
    - total_outcomes: int
    - total_activities: int
  </behavior>
  <implementation>
Create OutcomeValidator class following research patterns:

```python
from src.validators.validation_result import ValidationResult
from src.core.models import Course, Activity

class OutcomeValidator:
    """Validates learning outcome coverage and gap detection."""

    MIN_ACTIVITIES_PER_OUTCOME = 2  # Each outcome should map to 2+ activities

    def validate(self, course: Course) -> ValidationResult:
        """Run outcome coverage validation."""
        errors = []
        warnings = []
        suggestions = []

        # Get all activity IDs from course structure
        all_activities = self._flatten_activities(course)
        activity_ids = {a.id for a in all_activities}

        if not course.learning_outcomes:
            return ValidationResult(
                is_valid=True,
                errors=[],
                warnings=["No learning outcomes defined"],
                suggestions=["Add learning outcomes to track alignment"],
                metrics={
                    "coverage_score": 0.0,
                    "unmapped_outcomes": 0,
                    "low_coverage_outcomes": 0,
                    "unmapped_activities": len(all_activities),
                    "avg_activities_per_outcome": 0.0,
                    "total_outcomes": 0,
                    "total_activities": len(all_activities)
                }
            )

        # Check each outcome
        unmapped_outcomes = []
        low_coverage_outcomes = []

        for outcome in course.learning_outcomes:
            # Filter stale activity IDs
            valid_mappings = [aid for aid in outcome.mapped_activity_ids if aid in activity_ids]

            if len(valid_mappings) == 0:
                unmapped_outcomes.append(outcome)
            elif len(valid_mappings) < self.MIN_ACTIVITIES_PER_OUTCOME:
                low_coverage_outcomes.append((outcome, len(valid_mappings)))

        # ERROR: Unmapped outcomes
        if unmapped_outcomes:
            for outcome in unmapped_outcomes:
                behavior_preview = outcome.behavior[:50] + "..." if len(outcome.behavior) > 50 else outcome.behavior
                errors.append(f"Unmapped outcome: {behavior_preview}")

        # WARNING: Low coverage outcomes
        for outcome, count in low_coverage_outcomes:
            behavior_preview = outcome.behavior[:50] + "..." if len(outcome.behavior) > 50 else outcome.behavior
            warnings.append(
                f"Low coverage: '{behavior_preview}' has only {count} activity(ies) (recommended {self.MIN_ACTIVITIES_PER_OUTCOME}+)"
            )

        # Check for unmapped activities
        mapped_activity_ids = set()
        for outcome in course.learning_outcomes:
            mapped_activity_ids.update(outcome.mapped_activity_ids)

        unmapped_activities = [a for a in all_activities if a.id not in mapped_activity_ids]
        if unmapped_activities:
            warnings.append(f"{len(unmapped_activities)} activity(ies) not mapped to any learning outcome")

        # Calculate metrics
        covered_outcomes = len(course.learning_outcomes) - len(unmapped_outcomes)
        coverage_score = covered_outcomes / len(course.learning_outcomes)

        total_valid_mappings = sum(
            len([aid for aid in o.mapped_activity_ids if aid in activity_ids])
            for o in course.learning_outcomes
        )
        avg_activities = total_valid_mappings / len(course.learning_outcomes)

        metrics = {
            "coverage_score": round(coverage_score, 2),
            "unmapped_outcomes": len(unmapped_outcomes),
            "low_coverage_outcomes": len(low_coverage_outcomes),
            "unmapped_activities": len(unmapped_activities),
            "avg_activities_per_outcome": round(avg_activities, 1),
            "total_outcomes": len(course.learning_outcomes),
            "total_activities": len(all_activities)
        }

        return ValidationResult(
            is_valid=len(errors) == 0,
            errors=errors,
            warnings=warnings,
            suggestions=suggestions,
            metrics=metrics
        )

    def _flatten_activities(self, course: Course):
        """Extract all activities from course structure."""
        activities = []
        for module in course.modules:
            for lesson in module.lessons:
                activities.extend(lesson.activities)
        return activities
```
  </implementation>
</feature>

<verification>
RED phase:
- Write tests first that fail
- Run pytest tests/test_outcome_validator.py - tests must FAIL

GREEN phase:
- Implement OutcomeValidator
- Run pytest tests/test_outcome_validator.py - tests must PASS

REFACTOR phase (if needed):
- Clean up implementation
- Run pytest tests/test_outcome_validator.py - tests still PASS
</verification>

<success_criteria>
- OutcomeValidator.validate() returns ValidationResult
- Unmapped outcomes (0 activities) produce errors
- Low coverage outcomes (1 activity) produce warnings
- Unmapped activities produce warnings
- Stale activity IDs are filtered out (not counted)
- Metrics include coverage_score, counts
- All tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/07-validation-quality/07-02-SUMMARY.md`
</output>
