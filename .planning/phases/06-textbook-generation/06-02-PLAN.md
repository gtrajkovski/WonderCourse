---
phase: 06-textbook-generation
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/api/job_tracker.py
  - tests/test_job_tracker.py
autonomous: true

must_haves:
  truths:
    - "JobTracker can create a new job and return a unique task_id"
    - "JobTracker can update job progress, status, current_step, result, and error"
    - "JobTracker can retrieve job status by task_id"
    - "Jobs have lifecycle: pending -> running -> completed (or failed)"
    - "Non-existent task_id returns None from get_job"
  artifacts:
    - path: "src/api/job_tracker.py"
      provides: "In-memory job tracking for async generation"
      exports: ["JobTracker", "JobStatus"]
    - path: "tests/test_job_tracker.py"
      provides: "Job tracker unit tests"
      min_lines: 60
  key_links:
    - from: "src/api/job_tracker.py"
      to: "src/api/textbook.py"
      via: "imported by textbook API for progress tracking"
      pattern: "from src\\.api\\.job_tracker import JobTracker"
---

<objective>
Create an in-memory job tracking system for long-running generation tasks. The JobTracker provides task_id creation, progress updates, and status polling for the textbook generation pipeline.

Purpose: Textbook chapters take multiple API calls (outline + 5-8 sections + glossary + images + validation), requiring progress feedback to users. This provides the infrastructure for async generation with status polling.
Output: `src/api/job_tracker.py` with JobTracker class + comprehensive tests.
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement JobTracker with in-memory storage</name>
  <files>src/api/job_tracker.py</files>
  <action>
Create `src/api/job_tracker.py` with:

1. **JobStatus dataclass:**
   - `task_id: str`
   - `status: str` - one of "pending", "running", "completed", "failed"
   - `progress: float` - 0.0 to 1.0
   - `current_step: str` - human-readable description of current step
   - `result: Optional[dict]` - final result on completion (default None)
   - `error: Optional[str]` - error message on failure (default None)
   - `created_at: str` - ISO timestamp
   - `updated_at: str` - ISO timestamp
   - `to_dict() -> dict` method for JSON serialization

2. **JobTracker class** with class-level dict storage:
   - `_jobs: Dict[str, JobStatus] = {}` (class variable)
   - `create_job(cls, task_type: str) -> str` - Creates new job with task_id = f"{task_type}_{uuid.uuid4().hex[:8]}", status="pending", progress=0.0, current_step="Initializing". Returns task_id.
   - `update_job(cls, task_id: str, **kwargs) -> None` - Updates any fields on existing job. Sets updated_at automatically. No-op if task_id not found.
   - `get_job(cls, task_id: str) -> Optional[JobStatus]` - Returns job or None.
   - `clear_jobs(cls) -> None` - Clears all jobs (for testing).
   - All methods are `@classmethod`.

Use `from dataclasses import dataclass, field` for JobStatus.
Use `import uuid` and `from datetime import datetime`.
Use `from typing import Dict, Optional`.

Important: This is intentionally simple in-memory storage. No Redis, no Celery, no persistence. Migration path to Celery documented in research but deferred to Phase 8+.
  </action>
  <verify>
Run: `python -c "from src.api.job_tracker import JobTracker, JobStatus; tid = JobTracker.create_job('test'); print(f'Created: {tid}'); print(JobTracker.get_job(tid).to_dict())"` prints job details.
  </verify>
  <done>JobTracker class with create_job, update_job, get_job, clear_jobs methods working correctly with in-memory storage.</done>
</task>

<task type="auto">
  <name>Task 2: Write JobTracker tests</name>
  <files>tests/test_job_tracker.py</files>
  <action>
Create `tests/test_job_tracker.py` with these tests:

1. **test_create_job_returns_task_id** - create_job returns string starting with task_type prefix.
2. **test_create_job_initializes_status** - New job has status="pending", progress=0.0, current_step="Initializing".
3. **test_get_job_returns_job** - get_job returns the created job with correct task_id.
4. **test_get_job_returns_none_for_unknown** - get_job("nonexistent") returns None.
5. **test_update_job_changes_status** - update_job(tid, status="running") changes status.
6. **test_update_job_changes_progress** - update_job(tid, progress=0.5, current_step="Halfway") updates both fields.
7. **test_update_job_sets_result** - update_job(tid, status="completed", result={"data": "test"}) stores result.
8. **test_update_job_sets_error** - update_job(tid, status="failed", error="Something broke") stores error.
9. **test_update_job_updates_timestamp** - updated_at changes after update_job call.
10. **test_clear_jobs** - clear_jobs() removes all jobs, get_job returns None.
11. **test_job_status_to_dict** - to_dict() returns dict with all expected keys.

Use a fixture that calls `JobTracker.clear_jobs()` in setup/teardown to isolate tests.
  </action>
  <verify>Run: `python -m pytest tests/test_job_tracker.py -v` passes all 11 tests.</verify>
  <done>11 JobTracker tests passing, covering full lifecycle (create, update, get, clear) and edge cases.</done>
</task>

</tasks>

<verification>
- JobTracker importable from `src.api.job_tracker`
- All 11 tests pass
- JobStatus.to_dict() produces serializable dict
- Existing tests still pass: `python -m pytest` (300+ tests)
</verification>

<success_criteria>
- In-memory job tracking with create/update/get/clear operations
- Jobs track progress (0.0-1.0), status, current_step, result, and error
- Unique task_id generation with task_type prefix
- Tests verify full job lifecycle and edge cases
</success_criteria>

<output>
After completion, create `.planning/phases/06-textbook-generation/06-02-SUMMARY.md`
</output>
