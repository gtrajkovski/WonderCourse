---
phase: 05-extended-content-generation
plan: 03
type: tdd
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/generators/coach_generator.py
  - tests/test_coach_generator.py
autonomous: true

must_haves:
  truths:
    - "CoachGenerator extends BaseGenerator[CoachSchema] and generates valid coach dialogues"
    - "Generated dialogue has all 8 sections per specification"
    - "Sample responses include all 3 evaluation levels (exceeds, meets, needs_improvement)"
    - "Metadata includes word_count, section_count, content_type"
  artifacts:
    - path: "src/generators/coach_generator.py"
      provides: "CoachGenerator class"
      contains: "class CoachGenerator"
    - path: "tests/test_coach_generator.py"
      provides: "Tests for CoachGenerator"
      contains: "test_generate_returns_valid_schema"
  key_links:
    - from: "src/generators/coach_generator.py"
      to: "src/generators/base_generator.py"
      via: "class inheritance"
      pattern: "class CoachGenerator\\(BaseGenerator\\[CoachSchema\\]\\)"
---

<objective>
Create CoachGenerator with TDD for AI coach dialogue activities with 8-section structure and 3-level evaluation.

Purpose: Enable generation of interactive coach dialogues that guide student learning through structured conversations.
Output: Working CoachGenerator with tests
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@src/generators/base_generator.py
@src/generators/quiz_generator.py
@tests/test_quiz_generator.py
@src/generators/schemas/coach.py
</context>

<feature>
  <name>CoachGenerator</name>
  <files>src/generators/coach_generator.py, tests/test_coach_generator.py</files>
  <behavior>
    CoachGenerator extends BaseGenerator[CoachSchema].

    - system_prompt: Expert in AI-powered conversational coaching for education. Guidelines for 8-section structure: learning objectives, scenario, tasks, conversation starters, sample responses (3 levels), evaluation criteria, wrap-up, reflection prompts. Focus on Socratic dialogue, not lecturing.
    - build_user_prompt(learning_objective, topic, difficulty="intermediate"): Returns formatted prompt requesting coach dialogue with all 8 sections.
    - extract_metadata(content: CoachSchema) -> dict: Returns word_count (sum all text fields including nested), num_conversation_starters, num_sample_responses, num_evaluation_criteria, content_type ("coach").
    - generate_dialogue() convenience method wrapping generate(schema=CoachSchema, ...).

    Test cases:
    1. test_generate_returns_valid_schema - Mock API, verify CoachSchema with all 8 sections populated
    2. test_sample_responses_cover_all_levels - Verify 3 sample responses at exceeds/meets/needs_improvement
    3. test_system_prompt_mentions_eight_sections - System prompt references all 8 sections
    4. test_build_user_prompt_includes_params - Prompt includes learning_objective, topic
    5. test_extract_metadata_counts_sections - Metadata has correct counts
    6. test_api_called_with_output_config - Verify output_config parameter
  </behavior>
  <implementation>
    Follow quiz_generator.py pattern. Create sample coach JSON fixture with all 8 sections filled. Mock Anthropic via mocker.patch.
  </implementation>
</feature>

<verification>
`pytest tests/test_coach_generator.py -v` -- all 6 tests pass
</verification>

<success_criteria>
- CoachGenerator importable and extends BaseGenerator[CoachSchema]
- All 6 tests pass with mocked API
- All 8 sections present in generated dialogue
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-content-generation/05-03-SUMMARY.md`
</output>
