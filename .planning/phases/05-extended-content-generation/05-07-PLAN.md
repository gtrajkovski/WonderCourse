---
phase: 05-extended-content-generation
plan: 07
type: tdd
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/generators/assignment_generator.py
  - tests/test_assignment_generator.py
autonomous: true

must_haves:
  truths:
    - "AssignmentGenerator extends BaseGenerator[AssignmentSchema] and generates assignment specifications"
    - "Generated assignment has deliverables with points, grading criteria, and submission checklist"
    - "Metadata includes word_count, estimated_duration_minutes, total_points, content_type"
  artifacts:
    - path: "src/generators/assignment_generator.py"
      provides: "AssignmentGenerator class"
      contains: "class AssignmentGenerator"
    - path: "tests/test_assignment_generator.py"
      provides: "Tests for AssignmentGenerator"
      contains: "test_generate_returns_valid_schema"
  key_links:
    - from: "src/generators/assignment_generator.py"
      to: "src/generators/base_generator.py"
      via: "class inheritance"
      pattern: "class AssignmentGenerator\\(BaseGenerator\\[AssignmentSchema\\]\\)"
---

<objective>
Create AssignmentGenerator with TDD for assignment specifications with deliverables, grading criteria, and submission checklists.

Purpose: Enable generation of standalone assignment specs with clear expectations and grading rubrics.
Output: Working AssignmentGenerator with tests
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@src/generators/base_generator.py
@src/generators/quiz_generator.py
@tests/test_quiz_generator.py
@src/generators/schemas/assignment.py
</context>

<feature>
  <name>AssignmentGenerator</name>
  <files>src/generators/assignment_generator.py, tests/test_assignment_generator.py</files>
  <behavior>
    AssignmentGenerator extends BaseGenerator[AssignmentSchema].

    - system_prompt: Expert assessment designer for higher education assignments. Guidelines: clear deliverables with point values, actionable grading criteria, submission checklist to reduce incomplete submissions, realistic time estimates, alignment with learning objectives.
    - build_user_prompt(learning_objective, topic, total_points=100, estimated_hours=5, difficulty="intermediate"): Returns formatted prompt for assignment with deliverables, grading criteria, and checklist.
    - extract_metadata(content: AssignmentSchema) -> dict: Returns word_count (sum all text fields), estimated_duration_minutes (content.estimated_hours * 60), total_points (content.total_points), num_deliverables, content_type ("assignment").
    - generate_assignment() convenience method.

    Test cases:
    1. test_generate_returns_valid_schema - Mock API, verify AssignmentSchema with deliverables and checklist
    2. test_deliverables_have_points - Each deliverable has points >= 0
    3. test_checklist_has_required_field - Each checklist item has required boolean
    4. test_system_prompt_mentions_grading - System prompt references grading criteria
    5. test_build_user_prompt_includes_params - Prompt includes learning_objective, total_points, estimated_hours
    6. test_extract_metadata_includes_total_points - Metadata includes total_points from schema
  </behavior>
  <implementation>
    Follow quiz_generator.py pattern. Create sample assignment JSON fixture with deliverables, criteria, and checklist. Mock Anthropic via mocker.patch.
  </implementation>
</feature>

<verification>
`pytest tests/test_assignment_generator.py -v` -- all 6 tests pass
</verification>

<success_criteria>
- AssignmentGenerator importable and extends BaseGenerator[AssignmentSchema]
- All 6 tests pass with mocked API
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-content-generation/05-07-SUMMARY.md`
</output>
