---
phase: 05-extended-content-generation
plan: 06
type: tdd
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/generators/discussion_generator.py
  - tests/test_discussion_generator.py
autonomous: true

must_haves:
  truths:
    - "DiscussionGenerator extends BaseGenerator[DiscussionSchema] and generates discussion prompts"
    - "Generated discussion has main prompt, facilitation questions, and engagement hooks"
    - "Metadata includes word_count, num_facilitation_questions, num_engagement_hooks, content_type"
  artifacts:
    - path: "src/generators/discussion_generator.py"
      provides: "DiscussionGenerator class"
      contains: "class DiscussionGenerator"
    - path: "tests/test_discussion_generator.py"
      provides: "Tests for DiscussionGenerator"
      contains: "test_generate_returns_valid_schema"
  key_links:
    - from: "src/generators/discussion_generator.py"
      to: "src/generators/base_generator.py"
      via: "class inheritance"
      pattern: "class DiscussionGenerator\\(BaseGenerator\\[DiscussionSchema\\]\\)"
---

<objective>
Create DiscussionGenerator with TDD for discussion prompts with facilitation questions and engagement hooks.

Purpose: Enable generation of discussion activities that foster peer learning and instructor-guided dialogue.
Output: Working DiscussionGenerator with tests
</objective>

<execution_context>
@C:\Users\gpt30\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\gpt30\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/STATE.md
@src/generators/base_generator.py
@src/generators/quiz_generator.py
@tests/test_quiz_generator.py
@src/generators/schemas/discussion.py
</context>

<feature>
  <name>DiscussionGenerator</name>
  <files>src/generators/discussion_generator.py, tests/test_discussion_generator.py</files>
  <behavior>
    DiscussionGenerator extends BaseGenerator[DiscussionSchema].

    - system_prompt: Expert in facilitating online discussions in educational settings. Guidelines: open-ended prompts that spark diverse perspectives, connect to real-world experiences, encourage peer interaction not just instructor response. Facilitation questions guide instructors. Engagement hooks spark initial responses.
    - build_user_prompt(learning_objective, topic, difficulty="intermediate"): Returns formatted prompt for discussion with main prompt, facilitation questions, engagement hooks, and connection to objective.
    - extract_metadata(content: DiscussionSchema) -> dict: Returns word_count (sum all text fields), num_facilitation_questions, num_engagement_hooks, content_type ("discussion").
    - generate_discussion() convenience method.

    Test cases:
    1. test_generate_returns_valid_schema - Mock API, verify DiscussionSchema with all fields
    2. test_has_facilitation_questions - At least 3 facilitation questions present
    3. test_has_engagement_hooks - At least 2 engagement hooks present
    4. test_system_prompt_mentions_peer_learning - System prompt references peer interaction/learning
    5. test_build_user_prompt_includes_params - Prompt includes learning_objective, topic
    6. test_extract_metadata_counts_correctly - Metadata counts match list lengths
  </behavior>
  <implementation>
    Follow quiz_generator.py pattern. Create sample discussion JSON fixture. Mock Anthropic via mocker.patch.
  </implementation>
</feature>

<verification>
`pytest tests/test_discussion_generator.py -v` -- all 6 tests pass
</verification>

<success_criteria>
- DiscussionGenerator importable and extends BaseGenerator[DiscussionSchema]
- All 6 tests pass with mocked API
</success_criteria>

<output>
After completion, create `.planning/phases/05-extended-content-generation/05-06-SUMMARY.md`
</output>
